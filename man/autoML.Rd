% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autoML.R
\name{autoML}
\alias{autoML}
\title{Automated machine learning}
\usage{
autoML(train, test = NULL, target = NULL, id = NULL,
  removeDupObs = TRUE, clipOutliers = TRUE, trackingFeatures = TRUE,
  featureTransformations = TRUE, featureInteractions = TRUE,
  unsupervisedFeatures = TRUE, modelInterpretability = FALSE,
  trainMode = "reduced", topFeatures = 30, models = "all",
  tuneIters = 10, tuneType = "random", performResampling = FALSE,
  resampleMethod = "CV", resampleIters = 5, sample = NULL,
  maxObs = 40000, testSplit = 0.1, validationSplit = 0.3,
  problemType = NULL, clusters = NULL, perfMetric = "auto",
  seed = 1991, verbose = TRUE)
}
\arguments{
\item{train}{[data.frame | Required] Dataset to perform cleaning, engineering and training of models on, should contain target feature if supervised learning should be executed}

\item{test}{[data.frame | Optional] Optional testing set to validate models on. If none is provided, one will be created internally. Default of NULL}

\item{target}{[character | Optional] If a target is provided classification or regression models will be trained, if left as NULL unsupervised models will be trained. Default of NULL}

\item{id}{[character | Optioanl] ID features are automatically detected and removed from cleaning and engieering, the dataset is also de-duplicated accoring to the ID features specified. Default of NULL which will not de-duplicate, options are NULL, auto which automatically searches for ID feature, and manual specification. For best performance specify ID features}

\item{removeDupObs}{[character | Optional] Should duplicate observations be removed using the ID features detected or specified. Default of TRUE}

\item{clipOutliers}{[logical | Optional] Should outliers be clipped by the median value. Default of TRUE}

\item{trackingFeatures}{[logical | Optional] Should tracking features be created when cleaning the data. Useful for tree based models. Default of TRUE}

\item{featureTransformations}{[logical | Optional] Shoud feature transformations be computed for numeric and integer features, log and square-root transformations are used. Default of TRUE}

\item{featureInteractions}{[logical | Optional] Should feature interactions be computed for numeric and integer features. Default of TRUE}

\item{unsupervisedFeatures}{[logical | Optional] Should unsupervised features be cretead for numeric and integer feature. Uses k-means to create clusters on a feature and then calculates the distance to the center which is the final feature. Default of TRUE}

\item{modelInterpretability}{[logical | Optional] Should model interpretability plots be produced for supervised models}

\item{trainMode}{[character | Optional] Specifies how to train models, options are: all, full, reduced, balanced, reducedBalanced. all will use all of the other options when suitable. full trains models on all features. reduced trains models on top n features selected by a random forest. balanced trains models on all features but with oversampling the target to 50/50 proportion when the target is binary. reducedBalanced uses the top features as well as balancing the target when the target is binary. Either one or many options can be specified}

\item{topFeatures}{[integer | Optional] Top performing features as identified by the random forest model and used in the reduced training methods. Default of 30, if the training set has less than 30 features 50 percent of the top features will be used}

\item{models}{[character | Optional] Which models to train. Default of all. Available models can be seen by calling availableLearners. Either one or many options can be specified}

\item{tuneIters}{[integer | Optional] Number of tuning iterations to search for optimal hyper parameters. Default of 10}

\item{tuneType}{[character | Optional] Tune method applied, options are: random and frace. random uses random tuning and frace uses iterated f-racing algorithm for the best solution. Default of random}

\item{performResampling}{[logical | Optional] Should resampling be performed after tuning of the model have taken place. Default of FALSE}

\item{resampleMethod}{[character | Optional] Should resampling be performed, specifies the resampling method, options are: CV, Bootstrap}

\item{resampleIters}{[integer | Optional] Number of folds or bootstrap iterations to validate the model on}

\item{sample}{[numeric | Optional] Number between 0 and 1 to sample observations for faster model interpretability. Default of NULL which will use 1000 observations}

\item{maxObs}{[integer | Optional] Number of observations in the experiment training set on which models are trained, tuned and resampled on. Default of 40000. If the training set has less than 40k observations all will be used}

\item{testSplit}{[numeric | Optional] Percentage of data to allocate to the test set. Stratified sampling is done. Default of 0.1}

\item{validationSplit}{[numeric | Optional] Percentage of data to allocate to the validation set. Stratified sampling is done. Default of 0.3}

\item{problemType}{[character | Optional] Machine learning problem type, options are: binary, multi, regression and cluster. If left as NULL but target feature provided, problem type is automatically detected. Default of NULL}

\item{clusters}{[integer | Optional] For unsupervised problems, the number of clusters to optimize for. Default of NULL which will search for the best optimized number of clusters}

\item{perfMetric}{[character | Optional] Optimization metric on which to train and validate the model. Default of NULL wuill automatically select a metric, else for avaialble metrics use the function availableMetrcs}

\item{seed}{[integer | Optional] Random number seed for reproducible results}

\item{verbose}{[logical | Optional] Chatty function or not. Default of TRUE}
}
\value{
List containing training results and trained models
}
\description{
Automated machine learning for supervised and unsupervised problems. Supervised problems include binary classification, multi-class classification and regression.
}
\examples{
res <- autoML(train = iris, target = "Species", trainMode = "full", models = "xgboost")
}
\author{
Xander Horn
}
